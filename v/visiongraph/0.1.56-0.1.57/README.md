# Comparing `tmp/visiongraph-0.1.56-py3-none-any.whl.zip` & `tmp/visiongraph-0.1.57-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,289 +1,291 @@
-Zip file size: 260243 bytes, number of entries: 287
--rw-r--r--  2.0 unx     1758 b- defN 24-Apr-03 21:09 visiongraph/AsyncGraphNode.py
--rw-r--r--  2.0 unx     2806 b- defN 24-Apr-03 21:09 visiongraph/BaseGraph.py
--rw-r--r--  2.0 unx      678 b- defN 24-Apr-03 21:09 visiongraph/GraphNode.py
--rw-r--r--  2.0 unx      276 b- defN 24-Apr-03 21:09 visiongraph/Processable.py
--rw-r--r--  2.0 unx     2094 b- defN 24-Apr-03 21:09 visiongraph/VisionGraph.py
--rw-r--r--  2.0 unx     1898 b- defN 24-Apr-03 21:09 visiongraph/VisionGraphBuilder.py
--rw-r--r--  2.0 unx    25160 b- defN 24-Apr-03 21:09 visiongraph/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/cache/__init__.py
--rw-r--r--  2.0 unx      368 b- defN 24-Apr-03 21:09 visiongraph/data/Asset.py
--rw-r--r--  2.0 unx      376 b- defN 24-Apr-03 21:09 visiongraph/data/LocalAsset.py
--rw-r--r--  2.0 unx     1099 b- defN 24-Apr-03 21:09 visiongraph/data/RepositoryAsset.py
--rw-r--r--  2.0 unx      386 b- defN 24-Apr-03 21:09 visiongraph/data/__init__.py
--rw-r--r--  2.0 unx     2568 b- defN 24-Apr-03 21:09 visiongraph/data/labels/COCO.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/data/labels/__init__.py
--rw-r--r--  2.0 unx      172 b- defN 24-Apr-03 21:09 visiongraph/dsp/BaseFilterNumpy.py
--rw-r--r--  2.0 unx     2421 b- defN 24-Apr-03 21:09 visiongraph/dsp/LandmarkSmoothFilter.py
--rw-r--r--  2.0 unx     1487 b- defN 24-Apr-03 21:09 visiongraph/dsp/OneEuroFilter.py
--rw-r--r--  2.0 unx     1651 b- defN 24-Apr-03 21:09 visiongraph/dsp/OneEuroFilterNumba.py
--rw-r--r--  2.0 unx     2642 b- defN 24-Apr-03 21:09 visiongraph/dsp/OneEuroFilterNumpy.py
--rw-r--r--  2.0 unx     1037 b- defN 24-Apr-03 21:09 visiongraph/dsp/VectorNumpySmoothFilter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/dsp/__init__.py
--rw-r--r--  2.0 unx      770 b- defN 24-Apr-03 21:09 visiongraph/estimator/BaseClassifier.py
--rw-r--r--  2.0 unx      397 b- defN 24-Apr-03 21:09 visiongraph/estimator/BaseEstimator.py
--rw-r--r--  2.0 unx     5398 b- defN 24-Apr-03 21:09 visiongraph/estimator/BaseVisionEngine.py
--rw-r--r--  2.0 unx     1100 b- defN 24-Apr-03 21:09 visiongraph/estimator/ChainEstimator.py
--rw-r--r--  2.0 unx      403 b- defN 24-Apr-03 21:09 visiongraph/estimator/ScoreThresholdEstimator.py
--rw-r--r--  2.0 unx      868 b- defN 24-Apr-03 21:09 visiongraph/estimator/VisionClassifier.py
--rw-r--r--  2.0 unx      410 b- defN 24-Apr-03 21:09 visiongraph/estimator/VisionEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 24-Apr-03 21:09 visiongraph/estimator/calculator/UndistortionCalculator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/calculator/__init__.py
--rw-r--r--  2.0 unx     1231 b- defN 24-Apr-03 21:09 visiongraph/estimator/embedding/LandmarkEmbedder.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/embedding/__init__.py
--rw-r--r--  2.0 unx     3367 b- defN 24-Apr-03 21:09 visiongraph/estimator/embedding/knn/BaseKNNClassifier.py
--rw-r--r--  2.0 unx     2182 b- defN 24-Apr-03 21:09 visiongraph/estimator/embedding/knn/FaissKNNClassifier.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/embedding/knn/__init__.py
--rw-r--r--  2.0 unx     1664 b- defN 24-Apr-03 21:09 visiongraph/estimator/engine/InferenceEngineFactory.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/engine/__init__.py
--rw-r--r--  2.0 unx      529 b- defN 24-Apr-03 21:09 visiongraph/estimator/inpaint/BaseInpainter.py
--rw-r--r--  2.0 unx     2140 b- defN 24-Apr-03 21:09 visiongraph/estimator/inpaint/GMCNNInpainter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/inpaint/__init__.py
--rw-r--r--  2.0 unx     4104 b- defN 24-Apr-03 21:09 visiongraph/estimator/onnx/ONNXVisionEngine.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/onnx/__init__.py
--rw-r--r--  2.0 unx     3298 b- defN 24-Apr-03 21:09 visiongraph/estimator/openvino/OpenVinoEngine.py
--rw-r--r--  2.0 unx     2516 b- defN 24-Apr-03 21:09 visiongraph/estimator/openvino/OpenVinoObjectDetector.py
--rw-r--r--  2.0 unx     2967 b- defN 24-Apr-03 21:09 visiongraph/estimator/openvino/OpenVinoPoseEstimator.py
--rw-r--r--  2.0 unx      641 b- defN 24-Apr-03 21:09 visiongraph/estimator/openvino/SyncInferencePipeline.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/openvino/__init__.py
--rw-r--r--  2.0 unx     1877 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/CenterNetDetector.py
--rw-r--r--  2.0 unx     3270 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/CrowdHumanDetector.py
--rw-r--r--  2.0 unx     1908 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/DETRDetector.py
--rw-r--r--  2.0 unx      544 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/InstanceSegmentationEstimator.py
--rw-r--r--  2.0 unx      523 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/LandmarkEstimator.py
--rw-r--r--  2.0 unx      524 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/ObjectDetector.py
--rw-r--r--  2.0 unx     1279 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/RoiEstimator.py
--rw-r--r--  2.0 unx     3251 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/SSDDetector.py
--rw-r--r--  2.0 unx     2082 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/SlidingWindowEstimator.py
--rw-r--r--  2.0 unx     1436 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/SpatialCascadeEstimator.py
--rw-r--r--  2.0 unx     3823 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/UltralyticsYOLODetector.py
--rw-r--r--  2.0 unx     3300 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/YOLODetector.py
--rw-r--r--  2.0 unx     2904 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/YOLOXE2EDetector.py
--rw-r--r--  2.0 unx     1635 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/YOLOv5Detector.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/__init__.py
--rw-r--r--  2.0 unx     3441 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/camera/ArUcoCameraPoseEstimator.py
--rw-r--r--  2.0 unx      850 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/camera/BoardCameraCalibrator.py
--rw-r--r--  2.0 unx     4276 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/camera/ChArUcoCalibrator.py
--rw-r--r--  2.0 unx     3145 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/camera/ChessboardCalibrator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/camera/__init__.py
--rw-r--r--  2.0 unx     1127 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/AdasFaceDetector.py
--rw-r--r--  2.0 unx      512 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/FaceDetector.py
--rw-r--r--  2.0 unx     3805 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/OpenVinoFaceDetector.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/__init__.py
--rw-r--r--  2.0 unx     1760 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/emotion/AffectNetEmotionClassifier.py
--rw-r--r--  2.0 unx     2038 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/emotion/FERPlusEmotionClassifier.py
--rw-r--r--  2.0 unx      450 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/emotion/FaceEmotionEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/emotion/__init__.py
--rw-r--r--  2.0 unx     1595 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/eye/EyeOpenClosedEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/eye/__init__.py
--rw-r--r--  2.0 unx      609 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/landmark/FaceLandmarkEstimator.py
--rw-r--r--  2.0 unx     2646 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/landmark/IrisDistanceCalculator.py
--rw-r--r--  2.0 unx     2204 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceDetector.py
--rw-r--r--  2.0 unx     2324 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceMeshEstimator.py
--rw-r--r--  2.0 unx     1811 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/landmark/RegressionLandmarkEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/landmark/__init__.py
--rw-r--r--  2.0 unx     1280 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/pose/AdasHeadPoseEstimator.py
--rw-r--r--  2.0 unx      315 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/pose/HeadPoseEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/pose/__init__.py
--rw-r--r--  2.0 unx     4265 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/recognition/FaceRecognitionEstimator.py
--rw-r--r--  2.0 unx     2803 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/recognition/FaceReidentificationEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/face/recognition/__init__.py
--rw-r--r--  2.0 unx      517 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/hand/HandDetector.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/hand/__init__.py
--rw-r--r--  2.0 unx      608 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/hand/landmark/HandLandmarkEstimator.py
--rw-r--r--  2.0 unx     2778 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/hand/landmark/MediaPipeHandEstimator.py
--rw-r--r--  2.0 unx     2330 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/hand/landmark/OpenPoseHandEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/hand/landmark/__init__.py
--rw-r--r--  2.0 unx     2219 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/AEPoseEstimator.py
--rw-r--r--  2.0 unx     5328 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/EfficientPoseEstimator.py
--rw-r--r--  2.0 unx     7134 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/KAPAOPoseEstimator.py
--rw-r--r--  2.0 unx     3869 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/LiteHRNetEstimator.py
--rw-r--r--  2.0 unx     2834 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/LitePoseEstimator.py
--rw-r--r--  2.0 unx     4302 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/MediaPipeHolisticEstimator.py
--rw-r--r--  2.0 unx     3069 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/MediaPipePoseEstimator.py
--rw-r--r--  2.0 unx     6641 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/MobileHumanPoseEstimator.py
--rw-r--r--  2.0 unx     9862 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/MobileNetV2PoseEstimator.py
--rw-r--r--  2.0 unx     4304 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/MoveNetPoseEstimator.py
--rw-r--r--  2.0 unx     1876 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/OpenPoseEstimator.py
--rw-r--r--  2.0 unx      518 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/PoseEstimator.py
--rw-r--r--  2.0 unx     2347 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/TopDownPoseEstimator.py
--rw-r--r--  2.0 unx     4451 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/YOLOv8PoseEstimator.py
--rw-r--r--  2.0 unx     6637 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/pose/__init__.py
--rw-r--r--  2.0 unx     3165 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/segmentation/MODNetEstimator.py
--rw-r--r--  2.0 unx     8687 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/segmentation/MaskRCNNEstimator.py
--rw-r--r--  2.0 unx     1989 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/segmentation/MediaPipeSelfieSegmentation.py
--rw-r--r--  2.0 unx     3287 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/segmentation/YolactEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/spatial/segmentation/__init__.py
--rw-r--r--  2.0 unx     1627 b- defN 24-Apr-03 21:09 visiongraph/estimator/translation/DeblurGANv2.py
--rw-r--r--  2.0 unx      314 b- defN 24-Apr-03 21:09 visiongraph/estimator/translation/DepthEstimator.py
--rw-r--r--  2.0 unx     3887 b- defN 24-Apr-03 21:09 visiongraph/estimator/translation/MBLLENEstimator.py
--rw-r--r--  2.0 unx     2898 b- defN 24-Apr-03 21:09 visiongraph/estimator/translation/MidasDepthEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/estimator/translation/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/external/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/external/intel/__init__.py
--rw-r--r--  2.0 unx     4342 b- defN 24-Apr-03 21:09 visiongraph/external/intel/performance_metrics.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/external/intel/adapters/__init__.py
--rw-r--r--  2.0 unx     5682 b- defN 24-Apr-03 21:09 visiongraph/external/intel/adapters/inference_adapter.py
--rw-r--r--  2.0 unx     5151 b- defN 24-Apr-03 21:09 visiongraph/external/intel/adapters/model_adapter.py
--rw-r--r--  2.0 unx     8056 b- defN 24-Apr-03 21:09 visiongraph/external/intel/adapters/openvino_adapter.py
--rw-r--r--  2.0 unx     6585 b- defN 24-Apr-03 21:09 visiongraph/external/intel/adapters/ovms_adapter.py
--rw-r--r--  2.0 unx     2794 b- defN 24-Apr-03 21:09 visiongraph/external/intel/adapters/utils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/__init__.py
--rw-r--r--  2.0 unx     7442 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/centernet.py
--rw-r--r--  2.0 unx     5700 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/detection_model.py
--rw-r--r--  2.0 unx     2992 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/detr.py
--rw-r--r--  2.0 unx    14421 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/hpe_associative_embedding.py
--rw-r--r--  2.0 unx     7247 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/image_model.py
--rw-r--r--  2.0 unx    12558 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/model.py
--rw-r--r--  2.0 unx    17789 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/open_pose.py
--rw-r--r--  2.0 unx     5874 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/ssd.py
--rw-r--r--  2.0 unx     5805 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/types.py
--rw-r--r--  2.0 unx     7260 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/utils.py
--rw-r--r--  2.0 unx    21407 b- defN 24-Apr-03 21:09 visiongraph/external/intel/models/yolo.py
--rw-r--r--  2.0 unx      154 b- defN 24-Apr-03 21:09 visiongraph/external/intel/pipelines/__init__.py
--rw-r--r--  2.0 unx     5407 b- defN 24-Apr-03 21:09 visiongraph/external/intel/pipelines/async_pipeline.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/external/midas/__init__.py
--rw-r--r--  2.0 unx     7868 b- defN 24-Apr-03 21:09 visiongraph/external/midas/transforms.py
--rw-r--r--  2.0 unx      193 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/__init__.py
--rw-r--r--  2.0 unx     1816 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/core.py
--rw-r--r--  2.0 unx      333 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/detector.py
--rw-r--r--  2.0 unx     1147 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/metrics.py
--rw-r--r--  2.0 unx     5402 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/model.py
--rw-r--r--  2.0 unx     3576 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/testing.py
--rw-r--r--  2.0 unx     2647 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/testing_viz.py
--rw-r--r--  2.0 unx    16512 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/tracker.py
--rw-r--r--  2.0 unx      652 b- defN 24-Apr-03 21:09 visiongraph/external/motpy/utils.py
--rw-r--r--  2.0 unx     5312 b- defN 24-Apr-03 21:09 visiongraph/external/motrackers/Track.py
--rw-r--r--  2.0 unx     7486 b- defN 24-Apr-03 21:09 visiongraph/external/motrackers/Tracker.py
--rw-r--r--  2.0 unx      184 b- defN 24-Apr-03 21:09 visiongraph/external/motrackers/__init__.py
--rw-r--r--  2.0 unx       83 b- defN 24-Apr-03 21:09 visiongraph/external/motrackers/utils/__init__.py
--rw-r--r--  2.0 unx     8978 b- defN 24-Apr-03 21:09 visiongraph/external/motrackers/utils/misc.py
--rw-r--r--  2.0 unx    16500 b- defN 24-Apr-03 21:09 visiongraph/input/AzureKinectInput.py
--rw-r--r--  2.0 unx     3855 b- defN 24-Apr-03 21:09 visiongraph/input/BaseCamera.py
--rw-r--r--  2.0 unx     2352 b- defN 24-Apr-03 21:09 visiongraph/input/BaseDepthCamera.py
--rw-r--r--  2.0 unx     1309 b- defN 24-Apr-03 21:09 visiongraph/input/BaseDepthInput.py
--rw-r--r--  2.0 unx     4232 b- defN 24-Apr-03 21:09 visiongraph/input/BaseInput.py
--rw-r--r--  2.0 unx     1721 b- defN 24-Apr-03 21:09 visiongraph/input/CamGearInput.py
--rw-r--r--  2.0 unx     1493 b- defN 24-Apr-03 21:09 visiongraph/input/ImageInput.py
--rw-r--r--  2.0 unx     8373 b- defN 24-Apr-03 21:09 visiongraph/input/Oak1Input.py
--rw-r--r--  2.0 unx    19066 b- defN 24-Apr-03 21:09 visiongraph/input/RealSenseInput.py
--rw-r--r--  2.0 unx     5032 b- defN 24-Apr-03 21:09 visiongraph/input/VideoCaptureInput.py
--rw-r--r--  2.0 unx     4453 b- defN 24-Apr-03 21:09 visiongraph/input/ZEDInput.py
--rw-r--r--  2.0 unx     1651 b- defN 24-Apr-03 21:09 visiongraph/input/__init__.py
--rw-r--r--  2.0 unx     1887 b- defN 24-Apr-03 21:09 visiongraph/model/CameraIntrinsics.py
--rw-r--r--  2.0 unx      101 b- defN 24-Apr-03 21:09 visiongraph/model/CameraStreamType.py
--rw-r--r--  2.0 unx      549 b- defN 24-Apr-03 21:09 visiongraph/model/DepthBuffer.py
--rw-r--r--  2.0 unx      266 b- defN 24-Apr-03 21:09 visiongraph/model/VisionEngineModelLayer.py
--rw-r--r--  2.0 unx      753 b- defN 24-Apr-03 21:09 visiongraph/model/VisionEngineOutput.py
--rw-r--r--  2.0 unx      135 b- defN 24-Apr-03 21:09 visiongraph/model/_ImportStub.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/model/__init__.py
--rw-r--r--  2.0 unx     4229 b- defN 24-Apr-03 21:09 visiongraph/model/geometry/BoundingBox2D.py
--rw-r--r--  2.0 unx      815 b- defN 24-Apr-03 21:09 visiongraph/model/geometry/Size2D.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/model/geometry/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 24-Apr-03 21:09 visiongraph/model/parameter/ArgumentConfigurable.py
--rw-r--r--  2.0 unx      186 b- defN 24-Apr-03 21:09 visiongraph/model/parameter/NamedParameter.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/model/parameter/__init__.py
--rw-r--r--  2.0 unx      399 b- defN 24-Apr-03 21:09 visiongraph/model/tracker/Trackable.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/model/tracker/__init__.py
--rw-r--r--  2.0 unx       78 b- defN 24-Apr-03 21:09 visiongraph/model/types/InputShapeOrder.py
--rw-r--r--  2.0 unx       99 b- defN 24-Apr-03 21:09 visiongraph/model/types/MediaPipePoseModelComplexity.py
--rw-r--r--  2.0 unx      440 b- defN 24-Apr-03 21:09 visiongraph/model/types/ModelPrecision.py
--rw-r--r--  2.0 unx      199 b- defN 24-Apr-03 21:09 visiongraph/model/types/RealSenseColorScheme.py
--rw-r--r--  2.0 unx      204 b- defN 24-Apr-03 21:09 visiongraph/model/types/RealSenseFilter.py
--rw-r--r--  2.0 unx     1180 b- defN 24-Apr-03 21:09 visiongraph/model/types/VideoCaptureBackend.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/model/types/__init__.py
--rw-r--r--  2.0 unx      905 b- defN 24-Apr-03 21:09 visiongraph/node/ApplyNode.py
--rw-r--r--  2.0 unx      753 b- defN 24-Apr-03 21:09 visiongraph/node/BreakpointNode.py
--rw-r--r--  2.0 unx      815 b- defN 24-Apr-03 21:09 visiongraph/node/CustomNode.py
--rw-r--r--  2.0 unx      903 b- defN 24-Apr-03 21:09 visiongraph/node/ExtractNode.py
--rw-r--r--  2.0 unx      525 b- defN 24-Apr-03 21:09 visiongraph/node/PassThroughNode.py
--rw-r--r--  2.0 unx      825 b- defN 24-Apr-03 21:09 visiongraph/node/SequenceNode.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/node/__init__.py
--rw-r--r--  2.0 unx     1676 b- defN 24-Apr-03 21:09 visiongraph/output/ImagePreview.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/output/__init__.py
--rw-r--r--  2.0 unx      965 b- defN 24-Apr-03 21:09 visiongraph/output/fbs/FrameBufferSharingServer.py
--rw-r--r--  2.0 unx     1427 b- defN 24-Apr-03 21:09 visiongraph/output/fbs/SpoutServer.py
--rw-r--r--  2.0 unx     1623 b- defN 24-Apr-03 21:09 visiongraph/output/fbs/SyphonServer.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/output/fbs/__init__.py
--rw-r--r--  2.0 unx      980 b- defN 24-Apr-03 21:09 visiongraph/recorder/AsyncFrameSetRecorder.py
--rw-r--r--  2.0 unx     1108 b- defN 24-Apr-03 21:09 visiongraph/recorder/BaseFrameRecorder.py
--rw-r--r--  2.0 unx     1143 b- defN 24-Apr-03 21:09 visiongraph/recorder/CV2VideoRecorder.py
--rw-r--r--  2.0 unx     1016 b- defN 24-Apr-03 21:09 visiongraph/recorder/FrameSetRecorder.py
--rw-r--r--  2.0 unx      905 b- defN 24-Apr-03 21:09 visiongraph/recorder/MoviePyVideoRecorder.py
--rw-r--r--  2.0 unx     1377 b- defN 24-Apr-03 21:09 visiongraph/recorder/VidGearVideoRecorder.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/recorder/__init__.py
--rw-r--r--  2.0 unx      564 b- defN 24-Apr-03 21:09 visiongraph/result/ArUcoCameraPose.py
--rw-r--r--  2.0 unx     1521 b- defN 24-Apr-03 21:09 visiongraph/result/ArUcoMarkerDetection.py
--rw-r--r--  2.0 unx      167 b- defN 24-Apr-03 21:09 visiongraph/result/BaseResult.py
--rw-r--r--  2.0 unx      525 b- defN 24-Apr-03 21:09 visiongraph/result/CameraPoseResult.py
--rw-r--r--  2.0 unx      378 b- defN 24-Apr-03 21:09 visiongraph/result/ClassificationResult.py
--rw-r--r--  2.0 unx     1709 b- defN 24-Apr-03 21:09 visiongraph/result/DepthMap.py
--rw-r--r--  2.0 unx      413 b- defN 24-Apr-03 21:09 visiongraph/result/EmbeddingResult.py
--rw-r--r--  2.0 unx      474 b- defN 24-Apr-03 21:09 visiongraph/result/HeadPoseResult.py
--rw-r--r--  2.0 unx      333 b- defN 24-Apr-03 21:09 visiongraph/result/ImageResult.py
--rw-r--r--  2.0 unx      534 b- defN 24-Apr-03 21:09 visiongraph/result/LandmarkEmbeddingResult.py
--rw-r--r--  2.0 unx     1165 b- defN 24-Apr-03 21:09 visiongraph/result/ResultAnnotator.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-03 21:09 visiongraph/result/ResultDict.py
--rw-r--r--  2.0 unx      466 b- defN 24-Apr-03 21:09 visiongraph/result/ResultList.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/result/__init__.py
--rw-r--r--  2.0 unx      779 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/CrowdHumanResult.py
--rw-r--r--  2.0 unx     1354 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/InstanceSegmentationResult.py
--rw-r--r--  2.0 unx     3838 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/LandmarkDetectionResult.py
--rw-r--r--  2.0 unx     3475 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/ObjectDetectionResult.py
--rw-r--r--  2.0 unx      878 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/SpatialCascadeResult.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/__init__.py
--rw-r--r--  2.0 unx      924 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/BlazeFace.py
--rw-r--r--  2.0 unx     2194 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/BlazeFaceMesh.py
--rw-r--r--  2.0 unx      702 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/EmotionClassificationResult.py
--rw-r--r--  2.0 unx      694 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/EyeOpenClosedResult.py
--rw-r--r--  2.0 unx      409 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/FaceDetectionResult.py
--rw-r--r--  2.0 unx      881 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/FaceLandmarkResult.py
--rw-r--r--  2.0 unx     1390 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/IrisDistanceResult.py
--rw-r--r--  2.0 unx      868 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/RegressionFace.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/face/__init__.py
--rw-r--r--  2.0 unx     2648 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/hand/BlazeHand.py
--rw-r--r--  2.0 unx      409 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/hand/HandDetectionResult.py
--rw-r--r--  2.0 unx     2590 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/hand/HandLandmarkResult.py
--rw-r--r--  2.0 unx       88 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/hand/Handedness.py
--rw-r--r--  2.0 unx      106 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/hand/OpenPoseHand.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/hand/__init__.py
--rw-r--r--  2.0 unx     3435 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/BlazePose.py
--rw-r--r--  2.0 unx     1266 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/BlazePoseSegmentation.py
--rw-r--r--  2.0 unx     2826 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/COCOOpenPose.py
--rw-r--r--  2.0 unx     2680 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/COCOPose.py
--rw-r--r--  2.0 unx     2337 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/EfficientPose.py
--rw-r--r--  2.0 unx     1726 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/HolisticPose.py
--rw-r--r--  2.0 unx     2738 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/MobileHumanPose.py
--rw-r--r--  2.0 unx     2963 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/PoseLandmarkResult.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/result/spatial/pose/__init__.py
--rw-r--r--  2.0 unx      492 b- defN 24-Apr-03 21:09 visiongraph/tracker/BaseObjectDetectionTracker.py
--rw-r--r--  2.0 unx     1908 b- defN 24-Apr-03 21:09 visiongraph/tracker/CentroidTracker.py
--rw-r--r--  2.0 unx     4906 b- defN 24-Apr-03 21:09 visiongraph/tracker/FlateTracker.py
--rw-r--r--  2.0 unx     2705 b- defN 24-Apr-03 21:09 visiongraph/tracker/MotpyTracker.py
--rw-r--r--  2.0 unx     3304 b- defN 24-Apr-03 21:09 visiongraph/tracker/ObjectAssignmentSolver.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/tracker/__init__.py
--rw-r--r--  2.0 unx     3105 b- defN 24-Apr-03 21:09 visiongraph/util/ArgUtils.py
--rw-r--r--  2.0 unx     1009 b- defN 24-Apr-03 21:09 visiongraph/util/CodeUtils.py
--rw-r--r--  2.0 unx      222 b- defN 24-Apr-03 21:09 visiongraph/util/CollectionUtils.py
--rw-r--r--  2.0 unx      314 b- defN 24-Apr-03 21:09 visiongraph/util/CommonArgs.py
--rw-r--r--  2.0 unx     4659 b- defN 24-Apr-03 21:09 visiongraph/util/DrawingUtils.py
--rw-r--r--  2.0 unx     3926 b- defN 24-Apr-03 21:09 visiongraph/util/ImageUtils.py
--rw-r--r--  2.0 unx     2608 b- defN 24-Apr-03 21:09 visiongraph/util/LinalgUtils.py
--rw-r--r--  2.0 unx      539 b- defN 24-Apr-03 21:09 visiongraph/util/LoggingUtils.py
--rw-r--r--  2.0 unx     1752 b- defN 24-Apr-03 21:09 visiongraph/util/MathUtils.py
--rw-r--r--  2.0 unx      608 b- defN 24-Apr-03 21:09 visiongraph/util/MediaPipeUtils.py
--rw-r--r--  2.0 unx     2731 b- defN 24-Apr-03 21:09 visiongraph/util/NetworkUtils.py
--rw-r--r--  2.0 unx      195 b- defN 24-Apr-03 21:09 visiongraph/util/OSUtils.py
--rw-r--r--  2.0 unx      700 b- defN 24-Apr-03 21:09 visiongraph/util/OpenVinoUtils.py
--rw-r--r--  2.0 unx     5639 b- defN 24-Apr-03 21:09 visiongraph/util/PoseUtils.py
--rw-r--r--  2.0 unx     1608 b- defN 24-Apr-03 21:09 visiongraph/util/ResultUtils.py
--rw-r--r--  2.0 unx     2102 b- defN 24-Apr-03 21:09 visiongraph/util/TimeUtils.py
--rw-r--r--  2.0 unx     3643 b- defN 24-Apr-03 21:09 visiongraph/util/VectorUtils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Apr-03 21:09 visiongraph/util/__init__.py
--rw-r--r--  2.0 unx    11923 b- defN 24-Apr-03 21:09 visiongraph-0.1.56.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-03 21:09 visiongraph-0.1.56.dist-info/WHEEL
--rw-r--r--  2.0 unx       20 b- defN 24-Apr-03 21:09 visiongraph-0.1.56.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 24-Apr-03 21:09 visiongraph-0.1.56.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    28820 b- defN 24-Apr-03 21:09 visiongraph-0.1.56.dist-info/RECORD
-287 files, 702815 bytes uncompressed, 212949 bytes compressed:  69.7%
+Zip file size: 263131 bytes, number of entries: 289
+-rw-r--r--  2.0 unx     1758 b- defN 24-Apr-17 14:36 visiongraph/AsyncGraphNode.py
+-rw-r--r--  2.0 unx     2806 b- defN 24-Apr-17 14:36 visiongraph/BaseGraph.py
+-rw-r--r--  2.0 unx      678 b- defN 24-Apr-17 14:36 visiongraph/GraphNode.py
+-rw-r--r--  2.0 unx      276 b- defN 24-Apr-17 14:36 visiongraph/Processable.py
+-rw-r--r--  2.0 unx     2094 b- defN 24-Apr-17 14:36 visiongraph/VisionGraph.py
+-rw-r--r--  2.0 unx     1898 b- defN 24-Apr-17 14:36 visiongraph/VisionGraphBuilder.py
+-rw-r--r--  2.0 unx    25589 b- defN 24-Apr-17 14:36 visiongraph/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/cache/__init__.py
+-rw-r--r--  2.0 unx      368 b- defN 24-Apr-17 14:36 visiongraph/data/Asset.py
+-rw-r--r--  2.0 unx      376 b- defN 24-Apr-17 14:36 visiongraph/data/LocalAsset.py
+-rw-r--r--  2.0 unx     1099 b- defN 24-Apr-17 14:36 visiongraph/data/RepositoryAsset.py
+-rw-r--r--  2.0 unx      386 b- defN 24-Apr-17 14:36 visiongraph/data/__init__.py
+-rw-r--r--  2.0 unx     2568 b- defN 24-Apr-17 14:36 visiongraph/data/labels/COCO.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/data/labels/__init__.py
+-rw-r--r--  2.0 unx      172 b- defN 24-Apr-17 14:36 visiongraph/dsp/BaseFilterNumpy.py
+-rw-r--r--  2.0 unx     2421 b- defN 24-Apr-17 14:36 visiongraph/dsp/LandmarkSmoothFilter.py
+-rw-r--r--  2.0 unx     1487 b- defN 24-Apr-17 14:36 visiongraph/dsp/OneEuroFilter.py
+-rw-r--r--  2.0 unx     1651 b- defN 24-Apr-17 14:36 visiongraph/dsp/OneEuroFilterNumba.py
+-rw-r--r--  2.0 unx     2642 b- defN 24-Apr-17 14:36 visiongraph/dsp/OneEuroFilterNumpy.py
+-rw-r--r--  2.0 unx     1037 b- defN 24-Apr-17 14:36 visiongraph/dsp/VectorNumpySmoothFilter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/dsp/__init__.py
+-rw-r--r--  2.0 unx      770 b- defN 24-Apr-17 14:36 visiongraph/estimator/BaseClassifier.py
+-rw-r--r--  2.0 unx      397 b- defN 24-Apr-17 14:36 visiongraph/estimator/BaseEstimator.py
+-rw-r--r--  2.0 unx     5398 b- defN 24-Apr-17 14:36 visiongraph/estimator/BaseVisionEngine.py
+-rw-r--r--  2.0 unx     1100 b- defN 24-Apr-17 14:36 visiongraph/estimator/ChainEstimator.py
+-rw-r--r--  2.0 unx      403 b- defN 24-Apr-17 14:36 visiongraph/estimator/ScoreThresholdEstimator.py
+-rw-r--r--  2.0 unx      868 b- defN 24-Apr-17 14:36 visiongraph/estimator/VisionClassifier.py
+-rw-r--r--  2.0 unx      410 b- defN 24-Apr-17 14:36 visiongraph/estimator/VisionEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 24-Apr-17 14:36 visiongraph/estimator/calculator/UndistortionCalculator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/calculator/__init__.py
+-rw-r--r--  2.0 unx     1231 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/LandmarkEmbedder.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/__init__.py
+-rw-r--r--  2.0 unx     3367 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/knn/BaseKNNClassifier.py
+-rw-r--r--  2.0 unx     2182 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/knn/FaissKNNClassifier.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/embedding/knn/__init__.py
+-rw-r--r--  2.0 unx     1664 b- defN 24-Apr-17 14:36 visiongraph/estimator/engine/InferenceEngineFactory.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/engine/__init__.py
+-rw-r--r--  2.0 unx      529 b- defN 24-Apr-17 14:36 visiongraph/estimator/inpaint/BaseInpainter.py
+-rw-r--r--  2.0 unx     2140 b- defN 24-Apr-17 14:36 visiongraph/estimator/inpaint/GMCNNInpainter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/inpaint/__init__.py
+-rw-r--r--  2.0 unx     4104 b- defN 24-Apr-17 14:36 visiongraph/estimator/onnx/ONNXVisionEngine.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/onnx/__init__.py
+-rw-r--r--  2.0 unx     3298 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/OpenVinoEngine.py
+-rw-r--r--  2.0 unx     2516 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/OpenVinoObjectDetector.py
+-rw-r--r--  2.0 unx     2967 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/OpenVinoPoseEstimator.py
+-rw-r--r--  2.0 unx      641 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/SyncInferencePipeline.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/openvino/__init__.py
+-rw-r--r--  2.0 unx     1877 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/CenterNetDetector.py
+-rw-r--r--  2.0 unx     3270 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/CrowdHumanDetector.py
+-rw-r--r--  2.0 unx     1908 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/DETRDetector.py
+-rw-r--r--  2.0 unx      544 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/InstanceSegmentationEstimator.py
+-rw-r--r--  2.0 unx      523 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/LandmarkEstimator.py
+-rw-r--r--  2.0 unx      524 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/ObjectDetector.py
+-rw-r--r--  2.0 unx     1279 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/RoiEstimator.py
+-rw-r--r--  2.0 unx     3251 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/SSDDetector.py
+-rw-r--r--  2.0 unx     2082 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/SlidingWindowEstimator.py
+-rw-r--r--  2.0 unx     1436 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/SpatialCascadeEstimator.py
+-rw-r--r--  2.0 unx     3823 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/UltralyticsYOLODetector.py
+-rw-r--r--  2.0 unx     3300 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/YOLODetector.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/YOLOXE2EDetector.py
+-rw-r--r--  2.0 unx     1635 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/YOLOv5Detector.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/__init__.py
+-rw-r--r--  2.0 unx     3441 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/ArUcoCameraPoseEstimator.py
+-rw-r--r--  2.0 unx      850 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/BoardCameraCalibrator.py
+-rw-r--r--  2.0 unx     4276 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/ChArUcoCalibrator.py
+-rw-r--r--  2.0 unx     3145 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/ChessboardCalibrator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/camera/__init__.py
+-rw-r--r--  2.0 unx     1127 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/AdasFaceDetector.py
+-rw-r--r--  2.0 unx      512 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/FaceDetector.py
+-rw-r--r--  2.0 unx     3805 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/OpenVinoFaceDetector.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/__init__.py
+-rw-r--r--  2.0 unx     1760 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/AffectNetEmotionClassifier.py
+-rw-r--r--  2.0 unx     2038 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/FERPlusEmotionClassifier.py
+-rw-r--r--  2.0 unx      450 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/FaceEmotionEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/emotion/__init__.py
+-rw-r--r--  2.0 unx     1595 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/eye/EyeOpenClosedEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/eye/__init__.py
+-rw-r--r--  2.0 unx      609 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/FaceLandmarkEstimator.py
+-rw-r--r--  2.0 unx     2646 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/IrisDistanceCalculator.py
+-rw-r--r--  2.0 unx     2204 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceDetector.py
+-rw-r--r--  2.0 unx     2324 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceMeshEstimator.py
+-rw-r--r--  2.0 unx     1811 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/RegressionLandmarkEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/landmark/__init__.py
+-rw-r--r--  2.0 unx     1280 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/pose/AdasHeadPoseEstimator.py
+-rw-r--r--  2.0 unx      315 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/pose/HeadPoseEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/pose/__init__.py
+-rw-r--r--  2.0 unx     4265 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/recognition/FaceRecognitionEstimator.py
+-rw-r--r--  2.0 unx     2803 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/recognition/FaceReidentificationEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/face/recognition/__init__.py
+-rw-r--r--  2.0 unx      517 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/HandDetector.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/__init__.py
+-rw-r--r--  2.0 unx      608 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/HandLandmarkEstimator.py
+-rw-r--r--  2.0 unx     2778 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/MediaPipeHandEstimator.py
+-rw-r--r--  2.0 unx     2330 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/OpenPoseHandEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/hand/landmark/__init__.py
+-rw-r--r--  2.0 unx     2219 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/AEPoseEstimator.py
+-rw-r--r--  2.0 unx     5328 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/EfficientPoseEstimator.py
+-rw-r--r--  2.0 unx     7134 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/KAPAOPoseEstimator.py
+-rw-r--r--  2.0 unx     3869 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/LiteHRNetEstimator.py
+-rw-r--r--  2.0 unx     2834 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/LitePoseEstimator.py
+-rw-r--r--  2.0 unx     4302 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MediaPipeHolisticEstimator.py
+-rw-r--r--  2.0 unx     3069 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MediaPipePoseEstimator.py
+-rw-r--r--  2.0 unx     6641 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MobileHumanPoseEstimator.py
+-rw-r--r--  2.0 unx     9862 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MobileNetV2PoseEstimator.py
+-rw-r--r--  2.0 unx     4304 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/MoveNetPoseEstimator.py
+-rw-r--r--  2.0 unx     1876 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/OpenPoseEstimator.py
+-rw-r--r--  2.0 unx      518 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/PoseEstimator.py
+-rw-r--r--  2.0 unx     2347 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/TopDownPoseEstimator.py
+-rw-r--r--  2.0 unx     4451 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/YOLOv8PoseEstimator.py
+-rw-r--r--  2.0 unx     6637 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/pose/__init__.py
+-rw-r--r--  2.0 unx     3165 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/MODNetEstimator.py
+-rw-r--r--  2.0 unx     8687 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/MaskRCNNEstimator.py
+-rw-r--r--  2.0 unx     1989 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/MediaPipeSelfieSegmentation.py
+-rw-r--r--  2.0 unx     3287 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/YolactEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/spatial/segmentation/__init__.py
+-rw-r--r--  2.0 unx     1627 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/DeblurGANv2.py
+-rw-r--r--  2.0 unx      314 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/DepthEstimator.py
+-rw-r--r--  2.0 unx     3887 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/MBLLENEstimator.py
+-rw-r--r--  2.0 unx     2898 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/MidasDepthEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/estimator/translation/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/intel/__init__.py
+-rw-r--r--  2.0 unx     4342 b- defN 24-Apr-17 14:36 visiongraph/external/intel/performance_metrics.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/__init__.py
+-rw-r--r--  2.0 unx     5682 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/inference_adapter.py
+-rw-r--r--  2.0 unx     5151 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/model_adapter.py
+-rw-r--r--  2.0 unx     8056 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/openvino_adapter.py
+-rw-r--r--  2.0 unx     6585 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/ovms_adapter.py
+-rw-r--r--  2.0 unx     2794 b- defN 24-Apr-17 14:36 visiongraph/external/intel/adapters/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/__init__.py
+-rw-r--r--  2.0 unx     7442 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/centernet.py
+-rw-r--r--  2.0 unx     5700 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/detection_model.py
+-rw-r--r--  2.0 unx     2992 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/detr.py
+-rw-r--r--  2.0 unx    14421 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/hpe_associative_embedding.py
+-rw-r--r--  2.0 unx     7247 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/image_model.py
+-rw-r--r--  2.0 unx    12558 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/model.py
+-rw-r--r--  2.0 unx    17789 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/open_pose.py
+-rw-r--r--  2.0 unx     5874 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/ssd.py
+-rw-r--r--  2.0 unx     5805 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/types.py
+-rw-r--r--  2.0 unx     7260 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/utils.py
+-rw-r--r--  2.0 unx    21407 b- defN 24-Apr-17 14:36 visiongraph/external/intel/models/yolo.py
+-rw-r--r--  2.0 unx      154 b- defN 24-Apr-17 14:36 visiongraph/external/intel/pipelines/__init__.py
+-rw-r--r--  2.0 unx     5407 b- defN 24-Apr-17 14:36 visiongraph/external/intel/pipelines/async_pipeline.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/external/midas/__init__.py
+-rw-r--r--  2.0 unx     7868 b- defN 24-Apr-17 14:36 visiongraph/external/midas/transforms.py
+-rw-r--r--  2.0 unx      193 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/__init__.py
+-rw-r--r--  2.0 unx     1816 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/core.py
+-rw-r--r--  2.0 unx      333 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/detector.py
+-rw-r--r--  2.0 unx     1147 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/metrics.py
+-rw-r--r--  2.0 unx     5402 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/model.py
+-rw-r--r--  2.0 unx     3576 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/testing.py
+-rw-r--r--  2.0 unx     2647 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/testing_viz.py
+-rw-r--r--  2.0 unx    16512 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/tracker.py
+-rw-r--r--  2.0 unx      652 b- defN 24-Apr-17 14:36 visiongraph/external/motpy/utils.py
+-rw-r--r--  2.0 unx     5312 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/Track.py
+-rw-r--r--  2.0 unx     7486 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/Tracker.py
+-rw-r--r--  2.0 unx      184 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/__init__.py
+-rw-r--r--  2.0 unx       83 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/utils/__init__.py
+-rw-r--r--  2.0 unx     8978 b- defN 24-Apr-17 14:36 visiongraph/external/motrackers/utils/misc.py
+-rw-r--r--  2.0 unx    17144 b- defN 24-Apr-17 14:36 visiongraph/input/AzureKinectInput.py
+-rw-r--r--  2.0 unx     3855 b- defN 24-Apr-17 14:36 visiongraph/input/BaseCamera.py
+-rw-r--r--  2.0 unx     3338 b- defN 24-Apr-17 14:36 visiongraph/input/BaseDepthCamera.py
+-rw-r--r--  2.0 unx     1309 b- defN 24-Apr-17 14:36 visiongraph/input/BaseDepthInput.py
+-rw-r--r--  2.0 unx     4232 b- defN 24-Apr-17 14:36 visiongraph/input/BaseInput.py
+-rw-r--r--  2.0 unx     1721 b- defN 24-Apr-17 14:36 visiongraph/input/CamGearInput.py
+-rw-r--r--  2.0 unx     9093 b- defN 24-Apr-17 14:36 visiongraph/input/DepthAIBaseInput.py
+-rw-r--r--  2.0 unx     1493 b- defN 24-Apr-17 14:36 visiongraph/input/ImageInput.py
+-rw-r--r--  2.0 unx      295 b- defN 24-Apr-17 14:36 visiongraph/input/Oak1Input.py
+-rw-r--r--  2.0 unx     7425 b- defN 24-Apr-17 14:36 visiongraph/input/OakDInput.py
+-rw-r--r--  2.0 unx    19468 b- defN 24-Apr-17 14:36 visiongraph/input/RealSenseInput.py
+-rw-r--r--  2.0 unx     5032 b- defN 24-Apr-17 14:36 visiongraph/input/VideoCaptureInput.py
+-rw-r--r--  2.0 unx     4814 b- defN 24-Apr-17 14:36 visiongraph/input/ZEDInput.py
+-rw-r--r--  2.0 unx     1744 b- defN 24-Apr-17 14:36 visiongraph/input/__init__.py
+-rw-r--r--  2.0 unx     1887 b- defN 24-Apr-17 14:36 visiongraph/model/CameraIntrinsics.py
+-rw-r--r--  2.0 unx      101 b- defN 24-Apr-17 14:36 visiongraph/model/CameraStreamType.py
+-rw-r--r--  2.0 unx      549 b- defN 24-Apr-17 14:36 visiongraph/model/DepthBuffer.py
+-rw-r--r--  2.0 unx      266 b- defN 24-Apr-17 14:36 visiongraph/model/VisionEngineModelLayer.py
+-rw-r--r--  2.0 unx      753 b- defN 24-Apr-17 14:36 visiongraph/model/VisionEngineOutput.py
+-rw-r--r--  2.0 unx      135 b- defN 24-Apr-17 14:36 visiongraph/model/_ImportStub.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/__init__.py
+-rw-r--r--  2.0 unx     4229 b- defN 24-Apr-17 14:36 visiongraph/model/geometry/BoundingBox2D.py
+-rw-r--r--  2.0 unx      815 b- defN 24-Apr-17 14:36 visiongraph/model/geometry/Size2D.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/geometry/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 24-Apr-17 14:36 visiongraph/model/parameter/ArgumentConfigurable.py
+-rw-r--r--  2.0 unx      186 b- defN 24-Apr-17 14:36 visiongraph/model/parameter/NamedParameter.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/parameter/__init__.py
+-rw-r--r--  2.0 unx      399 b- defN 24-Apr-17 14:36 visiongraph/model/tracker/Trackable.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/tracker/__init__.py
+-rw-r--r--  2.0 unx       78 b- defN 24-Apr-17 14:36 visiongraph/model/types/InputShapeOrder.py
+-rw-r--r--  2.0 unx       99 b- defN 24-Apr-17 14:36 visiongraph/model/types/MediaPipePoseModelComplexity.py
+-rw-r--r--  2.0 unx      440 b- defN 24-Apr-17 14:36 visiongraph/model/types/ModelPrecision.py
+-rw-r--r--  2.0 unx      199 b- defN 24-Apr-17 14:36 visiongraph/model/types/RealSenseColorScheme.py
+-rw-r--r--  2.0 unx      204 b- defN 24-Apr-17 14:36 visiongraph/model/types/RealSenseFilter.py
+-rw-r--r--  2.0 unx     1180 b- defN 24-Apr-17 14:36 visiongraph/model/types/VideoCaptureBackend.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/model/types/__init__.py
+-rw-r--r--  2.0 unx      905 b- defN 24-Apr-17 14:36 visiongraph/node/ApplyNode.py
+-rw-r--r--  2.0 unx      753 b- defN 24-Apr-17 14:36 visiongraph/node/BreakpointNode.py
+-rw-r--r--  2.0 unx      815 b- defN 24-Apr-17 14:36 visiongraph/node/CustomNode.py
+-rw-r--r--  2.0 unx      903 b- defN 24-Apr-17 14:36 visiongraph/node/ExtractNode.py
+-rw-r--r--  2.0 unx      525 b- defN 24-Apr-17 14:36 visiongraph/node/PassThroughNode.py
+-rw-r--r--  2.0 unx      825 b- defN 24-Apr-17 14:36 visiongraph/node/SequenceNode.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/node/__init__.py
+-rw-r--r--  2.0 unx     1676 b- defN 24-Apr-17 14:36 visiongraph/output/ImagePreview.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/output/__init__.py
+-rw-r--r--  2.0 unx      965 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/FrameBufferSharingServer.py
+-rw-r--r--  2.0 unx     1427 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/SpoutServer.py
+-rw-r--r--  2.0 unx     1623 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/SyphonServer.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/output/fbs/__init__.py
+-rw-r--r--  2.0 unx      980 b- defN 24-Apr-17 14:36 visiongraph/recorder/AsyncFrameSetRecorder.py
+-rw-r--r--  2.0 unx     1108 b- defN 24-Apr-17 14:36 visiongraph/recorder/BaseFrameRecorder.py
+-rw-r--r--  2.0 unx     1143 b- defN 24-Apr-17 14:36 visiongraph/recorder/CV2VideoRecorder.py
+-rw-r--r--  2.0 unx     1016 b- defN 24-Apr-17 14:36 visiongraph/recorder/FrameSetRecorder.py
+-rw-r--r--  2.0 unx      905 b- defN 24-Apr-17 14:36 visiongraph/recorder/MoviePyVideoRecorder.py
+-rw-r--r--  2.0 unx     1377 b- defN 24-Apr-17 14:36 visiongraph/recorder/VidGearVideoRecorder.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/recorder/__init__.py
+-rw-r--r--  2.0 unx      564 b- defN 24-Apr-17 14:36 visiongraph/result/ArUcoCameraPose.py
+-rw-r--r--  2.0 unx     1521 b- defN 24-Apr-17 14:36 visiongraph/result/ArUcoMarkerDetection.py
+-rw-r--r--  2.0 unx      167 b- defN 24-Apr-17 14:36 visiongraph/result/BaseResult.py
+-rw-r--r--  2.0 unx      525 b- defN 24-Apr-17 14:36 visiongraph/result/CameraPoseResult.py
+-rw-r--r--  2.0 unx      378 b- defN 24-Apr-17 14:36 visiongraph/result/ClassificationResult.py
+-rw-r--r--  2.0 unx     1709 b- defN 24-Apr-17 14:36 visiongraph/result/DepthMap.py
+-rw-r--r--  2.0 unx      413 b- defN 24-Apr-17 14:36 visiongraph/result/EmbeddingResult.py
+-rw-r--r--  2.0 unx      474 b- defN 24-Apr-17 14:36 visiongraph/result/HeadPoseResult.py
+-rw-r--r--  2.0 unx      333 b- defN 24-Apr-17 14:36 visiongraph/result/ImageResult.py
+-rw-r--r--  2.0 unx      534 b- defN 24-Apr-17 14:36 visiongraph/result/LandmarkEmbeddingResult.py
+-rw-r--r--  2.0 unx     1165 b- defN 24-Apr-17 14:36 visiongraph/result/ResultAnnotator.py
+-rw-r--r--  2.0 unx      483 b- defN 24-Apr-17 14:36 visiongraph/result/ResultDict.py
+-rw-r--r--  2.0 unx      466 b- defN 24-Apr-17 14:36 visiongraph/result/ResultList.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/__init__.py
+-rw-r--r--  2.0 unx      779 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/CrowdHumanResult.py
+-rw-r--r--  2.0 unx     1354 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/InstanceSegmentationResult.py
+-rw-r--r--  2.0 unx     3838 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/LandmarkDetectionResult.py
+-rw-r--r--  2.0 unx     3475 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/ObjectDetectionResult.py
+-rw-r--r--  2.0 unx      878 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/SpatialCascadeResult.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/__init__.py
+-rw-r--r--  2.0 unx      924 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/BlazeFace.py
+-rw-r--r--  2.0 unx     2194 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/BlazeFaceMesh.py
+-rw-r--r--  2.0 unx      702 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/EmotionClassificationResult.py
+-rw-r--r--  2.0 unx      694 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/EyeOpenClosedResult.py
+-rw-r--r--  2.0 unx      409 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/FaceDetectionResult.py
+-rw-r--r--  2.0 unx      881 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/FaceLandmarkResult.py
+-rw-r--r--  2.0 unx     1390 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/IrisDistanceResult.py
+-rw-r--r--  2.0 unx      868 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/RegressionFace.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/face/__init__.py
+-rw-r--r--  2.0 unx     2648 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/BlazeHand.py
+-rw-r--r--  2.0 unx      409 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/HandDetectionResult.py
+-rw-r--r--  2.0 unx     2590 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/HandLandmarkResult.py
+-rw-r--r--  2.0 unx       88 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/Handedness.py
+-rw-r--r--  2.0 unx      106 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/OpenPoseHand.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/hand/__init__.py
+-rw-r--r--  2.0 unx     3435 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/BlazePose.py
+-rw-r--r--  2.0 unx     1266 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/BlazePoseSegmentation.py
+-rw-r--r--  2.0 unx     2826 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/COCOOpenPose.py
+-rw-r--r--  2.0 unx     2680 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/COCOPose.py
+-rw-r--r--  2.0 unx     2337 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/EfficientPose.py
+-rw-r--r--  2.0 unx     1726 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/HolisticPose.py
+-rw-r--r--  2.0 unx     2738 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/MobileHumanPose.py
+-rw-r--r--  2.0 unx     2963 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/PoseLandmarkResult.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/result/spatial/pose/__init__.py
+-rw-r--r--  2.0 unx      492 b- defN 24-Apr-17 14:36 visiongraph/tracker/BaseObjectDetectionTracker.py
+-rw-r--r--  2.0 unx     1908 b- defN 24-Apr-17 14:36 visiongraph/tracker/CentroidTracker.py
+-rw-r--r--  2.0 unx     4906 b- defN 24-Apr-17 14:36 visiongraph/tracker/FlateTracker.py
+-rw-r--r--  2.0 unx     2705 b- defN 24-Apr-17 14:36 visiongraph/tracker/MotpyTracker.py
+-rw-r--r--  2.0 unx     3304 b- defN 24-Apr-17 14:36 visiongraph/tracker/ObjectAssignmentSolver.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/tracker/__init__.py
+-rw-r--r--  2.0 unx     3105 b- defN 24-Apr-17 14:36 visiongraph/util/ArgUtils.py
+-rw-r--r--  2.0 unx     1009 b- defN 24-Apr-17 14:36 visiongraph/util/CodeUtils.py
+-rw-r--r--  2.0 unx      222 b- defN 24-Apr-17 14:36 visiongraph/util/CollectionUtils.py
+-rw-r--r--  2.0 unx      314 b- defN 24-Apr-17 14:36 visiongraph/util/CommonArgs.py
+-rw-r--r--  2.0 unx     4659 b- defN 24-Apr-17 14:36 visiongraph/util/DrawingUtils.py
+-rw-r--r--  2.0 unx     3926 b- defN 24-Apr-17 14:36 visiongraph/util/ImageUtils.py
+-rw-r--r--  2.0 unx     2608 b- defN 24-Apr-17 14:36 visiongraph/util/LinalgUtils.py
+-rw-r--r--  2.0 unx      539 b- defN 24-Apr-17 14:36 visiongraph/util/LoggingUtils.py
+-rw-r--r--  2.0 unx     1752 b- defN 24-Apr-17 14:36 visiongraph/util/MathUtils.py
+-rw-r--r--  2.0 unx      608 b- defN 24-Apr-17 14:36 visiongraph/util/MediaPipeUtils.py
+-rw-r--r--  2.0 unx     2731 b- defN 24-Apr-17 14:36 visiongraph/util/NetworkUtils.py
+-rw-r--r--  2.0 unx      195 b- defN 24-Apr-17 14:36 visiongraph/util/OSUtils.py
+-rw-r--r--  2.0 unx      700 b- defN 24-Apr-17 14:36 visiongraph/util/OpenVinoUtils.py
+-rw-r--r--  2.0 unx     5639 b- defN 24-Apr-17 14:36 visiongraph/util/PoseUtils.py
+-rw-r--r--  2.0 unx     1608 b- defN 24-Apr-17 14:36 visiongraph/util/ResultUtils.py
+-rw-r--r--  2.0 unx     2102 b- defN 24-Apr-17 14:36 visiongraph/util/TimeUtils.py
+-rw-r--r--  2.0 unx     3643 b- defN 24-Apr-17 14:36 visiongraph/util/VectorUtils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-17 14:36 visiongraph/util/__init__.py
+-rw-r--r--  2.0 unx    11923 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/WHEEL
+-rw-r--r--  2.0 unx       20 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    29000 b- defN 24-Apr-17 14:36 visiongraph-0.1.57.dist-info/RECORD
+289 files, 714350 bytes uncompressed, 215551 bytes compressed:  69.8%
```

## zipnote {}

```diff
@@ -492,20 +492,26 @@
 
 Filename: visiongraph/input/BaseInput.py
 Comment: 
 
 Filename: visiongraph/input/CamGearInput.py
 Comment: 
 
+Filename: visiongraph/input/DepthAIBaseInput.py
+Comment: 
+
 Filename: visiongraph/input/ImageInput.py
 Comment: 
 
 Filename: visiongraph/input/Oak1Input.py
 Comment: 
 
+Filename: visiongraph/input/OakDInput.py
+Comment: 
+
 Filename: visiongraph/input/RealSenseInput.py
 Comment: 
 
 Filename: visiongraph/input/VideoCaptureInput.py
 Comment: 
 
 Filename: visiongraph/input/ZEDInput.py
@@ -840,23 +846,23 @@
 
 Filename: visiongraph/util/VectorUtils.py
 Comment: 
 
 Filename: visiongraph/util/__init__.py
 Comment: 
 
-Filename: visiongraph-0.1.56.dist-info/METADATA
+Filename: visiongraph-0.1.57.dist-info/METADATA
 Comment: 
 
-Filename: visiongraph-0.1.56.dist-info/WHEEL
+Filename: visiongraph-0.1.57.dist-info/WHEEL
 Comment: 
 
-Filename: visiongraph-0.1.56.dist-info/entry_points.txt
+Filename: visiongraph-0.1.57.dist-info/entry_points.txt
 Comment: 
 
-Filename: visiongraph-0.1.56.dist-info/top_level.txt
+Filename: visiongraph-0.1.57.dist-info/top_level.txt
 Comment: 
 
-Filename: visiongraph-0.1.56.dist-info/RECORD
+Filename: visiongraph-0.1.57.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## visiongraph/__init__.py

```diff
@@ -321,20 +321,32 @@
 from .input.BaseDepthCamera import BaseDepthCamera
 from .input.BaseDepthInput import BaseDepthInput
 from .input.BaseInput import BaseInput
 try:
     from .input.CamGearInput import CamGearInput
 except ModuleNotFoundError as ex:
     logging.info(f"Module CamGearInput not found")
+try:
+    from .input.DepthAIBaseInput import DepthAIBaseInput
+except ModuleNotFoundError as ex:
+    logging.info(f"Module DepthAIBaseInput not found")
 from .input.ImageInput import ImageInput
 try:
     from .input.Oak1Input import Oak1Input
 except ModuleNotFoundError as ex:
     logging.info(f"Module Oak1Input not found")
 try:
+    from .input.OakDInput import OakDFrameAlignment
+except ModuleNotFoundError as ex:
+    logging.info(f"Module OakDFrameAlignment not found")
+try:
+    from .input.OakDInput import OakDInput
+except ModuleNotFoundError as ex:
+    logging.info(f"Module OakDInput not found")
+try:
     from .input.RealSenseInput import RealSenseInput
 except ModuleNotFoundError as ex:
     logging.info(f"Module RealSenseInput not found")
 from .input.VideoCaptureInput import VideoCaptureInput
 try:
     from .input.ZEDInput import ZEDCapture
 except ModuleNotFoundError as ex:
```

## visiongraph/input/AzureKinectInput.py

```diff
@@ -1,22 +1,21 @@
 import logging
 from argparse import ArgumentParser, Namespace
-from typing import Optional, Tuple
+from typing import Optional
 
 import cv2
 import numpy as np
 import pyk4a
 from pyk4a import PyK4A, PyK4ACapture, Config, PyK4ARecord, PyK4APlayback, ImageFormat, CalibrationType
 
 from visiongraph.input.BaseDepthCamera import BaseDepthCamera
 from visiongraph.model.CameraStreamType import CameraStreamType
 from visiongraph.util import CommonArgs
 from visiongraph.util.ArgUtils import add_enum_choice_argument
 from visiongraph.util.CollectionUtils import default_value_dict
-from visiongraph.util.MathUtils import transform_coordinates, constrain
 from visiongraph.util.TimeUtils import current_millis
 
 
 class AzureKinectInput(BaseDepthCamera):
     _HeightToResolutionMapping = default_value_dict(pyk4a.ColorResolution.RES_720P,
                                                     {
                                                         720: pyk4a.ColorResolution.RES_720P,
@@ -67,14 +66,18 @@
         self.output_mkv_file: Optional[str] = None
 
         self._record: Optional[PyK4ARecord] = None
         self._playback: Optional[PyK4APlayback] = None
 
         self.loop: bool = True
 
+        self._last_color_frame: Optional[np.ndarray] = None
+        self._last_ir_frame: Optional[np.ndarray] = None
+        self._last_depth_frame: Optional[np.ndarray] = None
+
     def setup(self, config: Optional[Config] = None):
         if self.input_mkv_file is not None:
             logging.info(f"Playing mkv file from {self.input_mkv_file}")
             self._playback = PyK4APlayback(self.input_mkv_file)
             self._playback.open()
             self.color_format = self._playback.configuration["color_format"]
             return
@@ -132,23 +135,26 @@
 
         if self._record is not None:
             self._record.write_capture(self.capture)
 
         if self.enable_depth and self.use_depth_as_input:
             depth = self.capture.depth
             image = self._colorize(depth, (self.depth_min_clipping, self.depth_max_clipping), self.depth_color_map)
+            self._last_depth_frame = image
         else:
             if self.use_infrared:
                 ir_frame = self.capture.transformed_ir if self.align_frames_to_color else self.capture.ir
                 image = self._colorize(ir_frame, (self.ir_min_clipping, self.ir_max_clipping), self.ir_color_map)
+                self._last_ir_frame = image
             else:
                 image = self.capture.transformed_color if self.align_frames_to_depth else self.capture.color
                 if image is not None:
                     image = self._convert_to_bgra_if_required(self.color_format, image)
                     image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
+                self._last_color_frame = image
 
         if image is None:
             logging.warning("could not read frame.")
             return self._post_process(time_stamp, None)
 
         return self._post_process(time_stamp, image)
 
@@ -340,14 +346,24 @@
         calibration = self.playback.calibration if self.is_playback else self.device.calibration
         return calibration.get_camera_matrix(self._to_k4a_calibration_type(stream_type))
 
     def get_fisheye_distortion(self, stream_type: CameraStreamType = CameraStreamType.Color) -> np.ndarray:
         calibration = self.playback.calibration if self.is_playback else self.device.calibration
         return calibration.get_distortion_coefficients(self._to_k4a_calibration_type(stream_type))
 
+    def get_raw_image(self, stream_type: CameraStreamType = CameraStreamType.Color) -> Optional[np.ndarray]:
+        if stream_type == CameraStreamType.Depth:
+            return self._last_depth_frame
+        elif stream_type == CameraStreamType.Infrared:
+            return self._last_ir_frame
+        elif stream_type == CameraStreamType.Color:
+            return self._last_color_frame
+
+        return None
+
     @property
     def serial(self) -> str:
         return self.device.serial
 
     @property
     def color(self) -> np.ndarray:
         if self._playback is None:
```

## visiongraph/input/BaseDepthCamera.py

```diff
@@ -1,16 +1,17 @@
-from abc import ABC
+from abc import ABC, abstractmethod
 from argparse import ArgumentParser, Namespace, ArgumentError
 from typing import Tuple, Optional
 
 import cv2
 import numpy as np
 
 from visiongraph.input.BaseCamera import BaseCamera
 from visiongraph.input.BaseDepthInput import BaseDepthInput
+from visiongraph.model.CameraStreamType import CameraStreamType
 from visiongraph.util import MathUtils
 
 
 class BaseDepthCamera(BaseCamera, BaseDepthInput, ABC):
     def __init__(self):
         super().__init__()
 
@@ -50,14 +51,43 @@
             img = image
             img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
 
         if colormap is not None:
             img = cv2.applyColorMap(img, colormap)
         return img
 
+    @abstractmethod
+    def get_raw_image(self, stream_type: CameraStreamType = CameraStreamType.Color) -> Optional[np.ndarray]:
+        pass
+
+    def get_image(self, stream_type: CameraStreamType = CameraStreamType.Color,
+                  post_processed: bool = True) -> Optional[np.ndarray]:
+        frame = self.get_raw_image(stream_type)
+
+        if frame is None:
+            return None
+
+        if post_processed:
+            _, frame = self._post_process(0, frame)
+            return frame
+
+        return frame
+
+    @property
+    def color_image(self) -> Optional[np.ndarray]:
+        return self.get_image(CameraStreamType.Color, True)
+
+    @property
+    def depth_image(self) -> Optional[np.ndarray]:
+        return self.get_image(CameraStreamType.Depth, True)
+
+    @property
+    def infrared_image(self) -> Optional[np.ndarray]:
+        return self.get_image(CameraStreamType.Infrared, True)
+
     @staticmethod
     def add_params(parser: ArgumentParser):
         super(BaseDepthCamera, BaseDepthCamera).add_params(parser)
         BaseDepthInput.add_params(parser)
 
         try:
             parser.add_argument("-ir", "--infrared", action="store_true",
```

## visiongraph/input/Oak1Input.py

```diff
@@ -1,251 +1,11 @@
-import typing
-from datetime import timedelta
-from typing import Optional, Tuple
+from typing import Optional
 
-import depthai as dai
 import numpy as np
-from depthai import CameraFeatures
 
-from visiongraph.input.BaseCamera import BaseCamera
-from visiongraph.model.CameraStreamType import CameraStreamType
+from visiongraph.input.DepthAIBaseInput import DepthAIBaseInput
 
-_CameraProperties = dai.ColorCameraProperties
-
-
-class Oak1Input(BaseCamera):
-
-    def __init__(self):
-        super().__init__()
-
-        # settings
-        self.sensor_resolution: _CameraProperties.SensorResolution = _CameraProperties.SensorResolution.THE_1080_P
-
-        self.interleaved: bool = False
-        self.isp_scale: Optional[Tuple[int, int]] = None
-        self.camera_board_socket: dai.CameraBoardSocket = dai.CameraBoardSocket.CAM_A
-
-        self._focus_mode: dai.RawCameraControl.AutoFocusMode = dai.RawCameraControl.AutoFocusMode.AUTO
-        self._manual_lens_pos: int = 0
-
-        self._auto_exposure: bool = True
-        self._exposure: timedelta = timedelta(microseconds=30)
-        self._iso_sensitivity: int = 400
-
-        self._auto_white_balance: bool = True
-        self._white_balance: int = 1000
-
-        # pipeline objects
-        self.pipeline: Optional[dai.Pipeline] = None
-        self.cam: Optional[dai.node.ColorCamera] = None
-        self.device: Optional[dai.Device] = None
-
-        # node names
-        self.rgb_stream_name = "rgb"
-        self.isp_stream_name = "isp"
-        self.control_in_name = "control_in"
-
-        # nodes
-        self.x_out: Optional[dai.node.XLinkOut] = None
-        self.isp_out: Optional[dai.node.XLinkOut] = None
-        self.control_in: Optional[dai.node.XLinkIn] = None
-
-        self.control_queue: Optional[dai.DataInputQueue] = None
-        self.rgb_queue: Optional[dai.DataOutputQueue] = None
-        self.isp_queue: Optional[dai.DataOutputQueue] = None
-
-    def setup(self):
-        self.pipeline = dai.Pipeline()
-
-        self.cam = self.pipeline.create(dai.node.ColorCamera)
-        self.cam.setBoardSocket(self.camera_board_socket)
-        self.cam.setResolution(self.sensor_resolution)
-        # self.cam.setVideoSize(self.width, self.height)
-        self.cam.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)
-        self.cam.setInterleaved(self.interleaved)
-
-        if self.isp_scale is not None:
-            self.cam.setIspScale(self.isp_scale[0], self.isp_scale[1])
-
-        self.x_out = self.pipeline.create(dai.node.XLinkOut)
-        self.x_out.setStreamName(self.rgb_stream_name)
-        self.cam.video.link(self.x_out.input)
-
-        self.isp_out = self.pipeline.create(dai.node.XLinkOut)
-        self.isp_out.setStreamName(self.isp_stream_name)
-        self.cam.isp.link(self.isp_out.input)
-
-        self.control_in = self.pipeline.create(dai.node.XLinkIn)
-        self.control_in.setStreamName(self.control_in_name)
-        self.control_in.out.link(self.cam.inputControl)
-
-        # starts pipeline
-        self.device = dai.Device(self.pipeline)
-
-        self.control_queue = self.device.getInputQueue(self.control_in_name)
-        self.rgb_queue = self.device.getOutputQueue(name=self.rgb_stream_name, maxSize=1, blocking=False)
-        self.isp_queue = self.device.getOutputQueue(name=self.isp_stream_name, maxSize=1, blocking=False)
-
-        isp_frame = typing.cast(dai.ImgFrame, self.isp_queue.get())
-        self.width = isp_frame.getWidth()
-        self.height = isp_frame.getHeight()
 
+class Oak1Input(DepthAIBaseInput):
     def read(self) -> (int, Optional[np.ndarray]):
-        frame = typing.cast(dai.ImgFrame, self.rgb_queue.get())
-
-        # update frame information
-        self._manual_lens_pos = frame.getLensPosition()
-        self._exposure = frame.getExposureTime()
-        self._white_balance = frame.getColorTemperature()
-
-        ts = int(frame.getTimestamp().total_seconds() * 1000)
-        image = typing.cast(np.ndarray, frame.getCvFrame())
-
-        return self._post_process(ts, image)
-
-    def release(self):
-        self.device.close()
-
-    @property
-    def gain(self) -> int:
-        raise Exception("Gain is not supported.")
-
-    @gain.setter
-    def gain(self, value: int):
-        raise Exception("Gain is not supported.")
-
-    @property
-    def iso(self) -> int:
-        return self._iso_sensitivity
-
-    @iso.setter
-    def iso(self, value: int):
-        if not self.is_running:
-            return
-
-        self._iso_sensitivity = value
-
-        # trigger exposure to set value
-        self.exposure = self.exposure
-
-    @property
-    def exposure(self) -> int:
-        return int(self._exposure.total_seconds() * 1000 * 1000)
-
-    @exposure.setter
-    def exposure(self, value: int):
-        if not self.is_running:
-            return
-
-        ctrl = dai.CameraControl()
-        value = max(1, min(60 * 1000 * 1000, int(value)))
-        self._exposure = timedelta(microseconds=value)
-        ctrl.setManualExposure(self._exposure, self._iso_sensitivity)
-        self.control_queue.send(ctrl)
-
-    @property
-    def enable_auto_exposure(self) -> bool:
-        return self._auto_exposure
-
-    @enable_auto_exposure.setter
-    def enable_auto_exposure(self, value: bool):
-        if not self.is_running:
-            return
-
-        ctrl = dai.CameraControl()
-        self._auto_exposure = value
-        if value:
-            ctrl.setAutoExposureEnable()
-        else:
-            ctrl.setManualExposure(self._exposure, self._iso_sensitivity)
-        self.control_queue.send(ctrl)
-
-    @property
-    def enable_auto_white_balance(self) -> bool:
-        return self._auto_white_balance
-
-    @enable_auto_white_balance.setter
-    def enable_auto_white_balance(self, value: bool):
-        if not self.is_running:
-            return
-
-        ctrl = dai.CameraControl()
-        self._auto_white_balance = value
-        if value:
-            ctrl.setAutoWhiteBalanceMode(dai.RawCameraControl.AutoWhiteBalanceMode.AUTO)
-        else:
-            ctrl.setAutoWhiteBalanceMode(dai.RawCameraControl.AutoWhiteBalanceMode.OFF)
-        self.control_queue.send(ctrl)
-
-    @property
-    def white_balance(self) -> int:
-        return self._white_balance
-
-    @white_balance.setter
-    def white_balance(self, value: int):
-        if not self.is_running:
-            return
-
-        ctrl = dai.CameraControl()
-        value = max(1000, min(12000, int(value)))
-        ctrl.setManualWhiteBalance(value)
-        self.control_queue.send(ctrl)
-
-    @property
-    def auto_focus(self) -> bool:
-        return self._focus_mode == dai.RawCameraControl.AutoFocusMode.AUTO
-
-    @auto_focus.setter
-    def auto_focus(self, value: bool):
-        if not self.is_running:
-            return
-
-        ctrl = dai.CameraControl()
-        if value:
-            self._focus_mode = dai.RawCameraControl.AutoFocusMode.AUTO
-            ctrl.setAutoFocusMode(dai.RawCameraControl.AutoFocusMode.AUTO)
-            ctrl.setAutoFocusTrigger()
-        else:
-            self._focus_mode = dai.RawCameraControl.AutoFocusMode.OFF
-            ctrl.setAutoFocusMode(dai.RawCameraControl.AutoFocusMode.OFF)
-        self.control_queue.send(ctrl)
-
-    @property
-    def focus_distance(self) -> int:
-        return self._manual_lens_pos
-
-    @focus_distance.setter
-    def focus_distance(self, position: int):
-        if not self.is_running:
-            return
-
-        ctrl = dai.CameraControl()
-        position = max(0, min(255, int(position)))
-        ctrl.setManualFocus(position)
-        self.control_queue.send(ctrl)
-
-    def get_camera_matrix(self, stream_type: CameraStreamType = CameraStreamType.Color) -> np.ndarray:
-        calibration_data = self.device.readCalibration()
-        intrinsics = calibration_data.getCameraIntrinsics(self.camera_board_socket)
-        return np.array(intrinsics)
-
-    def get_fisheye_distortion(self, stream_type: CameraStreamType = CameraStreamType.Color) -> np.ndarray:
-        calibration_data = self.device.readCalibration()
-        distortion = calibration_data.getDistortionCoefficients(self.camera_board_socket)
-        return np.array(distortion)
-
-    @property
-    def serial(self) -> str:
-        info = self.device.getDeviceInfo()
-        return info.mxid
-
-    @property
-    def camera_features(self) -> typing.List[CameraFeatures]:
-        return self.device.getConnectedCameraFeatures()
-
-    @property
-    def device_info(self) -> dai.DeviceInfo:
-        return self.device.getDeviceInfo()
-
-    @property
-    def is_running(self):
-        return self.device is not None and self.device.isPipelineRunning()
+        super().read()
+        return self._post_process(self._last_ts, self._last_rgb_frame)
```

## visiongraph/input/RealSenseInput.py

```diff
@@ -256,16 +256,15 @@
         point = rs.rs2_deproject_pixel_to_point(depth_intrinsics, [ix, iy], distance)
         return vector.obj(x=point[0], y=point[1], z=point[2])
 
     @property
     def depth_map(self) -> np.ndarray:
         depth_frame = self.depth_frame
         depth_colormap = np.asanyarray(self.colorizer.colorize(depth_frame).get_data())
-        ts, transformed_depth = self._post_process(0, depth_colormap)
-        return transformed_depth
+        return depth_colormap
 
     @property
     def depth_buffer(self) -> np.ndarray:
         return np.asarray(self.depth_frame.data, dtype=float)
 
     def allow_any_stream(self):
         self.width = 0
@@ -344,14 +343,24 @@
                          [0, intrinsics.fy, intrinsics.ppy],
                          [0, 0, 1]])
 
     def get_fisheye_distortion(self, stream_type: CameraStreamType = CameraStreamType.Color) -> np.ndarray:
         intrinsics = self.get_realsense_intrinsics(self._to_rs2_stream_type(stream_type))
         return np.array(intrinsics.coeffs[:4])
 
+    def get_raw_image(self, stream_type: CameraStreamType = CameraStreamType.Color) -> Optional[np.ndarray]:
+        if stream_type == CameraStreamType.Depth:
+            return self.depth_map
+        elif stream_type == CameraStreamType.Infrared:
+            return np.asanyarray(self.frames.get_infrared_frame().get_data())
+        elif stream_type == CameraStreamType.Color:
+            return np.asanyarray(self.frames.get_color_frame().get_data())
+
+        return None
+
     @property
     def device_count(self) -> int:
         ctx = rs.context()
         return len(ctx.query_devices())
 
     def get_option(self, option: rs.option, sensor: Optional[rs.sensor] = None) -> float:
         if sensor is None:
```

## visiongraph/input/ZEDInput.py

```diff
@@ -3,15 +3,14 @@
 
 import cv2
 import numpy as np
 import pyzed.sl as sl
 
 from visiongraph.input.BaseDepthCamera import BaseDepthCamera
 from visiongraph.model.CameraStreamType import CameraStreamType
-from visiongraph.util.MathUtils import constrain
 
 
 class ZEDCapture:
     timestamp: float = 0
     left_image: sl.Mat = sl.Mat()
     depth: sl.Mat = sl.Mat()
 
@@ -29,14 +28,16 @@
         self.init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode DEPTH_MODE.PERFORMANCE
         self.init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)
 
         self.capture = ZEDCapture()
 
         self.runtime_parameters = sl.RuntimeParameters()
 
+        self._last_color_frame: Optional[np.ndarray] = None
+
     def setup(self):
         err = self.camera.open(self.init_params)
         if err != sl.ERROR_CODE.SUCCESS:
             raise Exception(f"Could not start ZED camera: {err}")
         print("camera has been opened")
 
     def read(self) -> (int, Optional[np.ndarray]):
@@ -46,14 +47,16 @@
 
         # fill capture
         self.capture.timestamp = self.camera.get_timestamp(sl.TIME_REFERENCE.CURRENT).get_milliseconds()
         self.camera.retrieve_image(self.capture.left_image, sl.VIEW.LEFT)
         self.camera.retrieve_measure(self.capture.depth, sl.MEASURE.DEPTH)
 
         frame = cv2.cvtColor(self.capture.left_image.get_data(), cv2.COLOR_BGRA2BGR)
+        self._last_color_frame = frame
+
         return 0, frame
 
     def release(self):
         self.camera.close()
 
     def distance(self, x: float, y: float) -> float:
         if not self.camera.is_opened():
@@ -140,7 +143,15 @@
 
     @property
     def serial(self) -> str:
         if not self.camera.is_opened():
             return "none"
 
         return str(self.camera.get_camera_information().serial_number)
+
+    def get_raw_image(self, stream_type: CameraStreamType = CameraStreamType.Color) -> Optional[np.ndarray]:
+        if stream_type == CameraStreamType.Depth:
+            return self.depth_map
+        elif stream_type == CameraStreamType.Color:
+            return self._last_color_frame
+
+        return None
```

## visiongraph/input/__init__.py

```diff
@@ -32,16 +32,18 @@
 
     InputProviders["camgear"] = CamGearInput
 except ImportError as ex:
     logging.info(f"VidGear not installed: {ex}")
 
 try:
     from visiongraph.input.Oak1Input import Oak1Input
+    from visiongraph.input.OakDInput import OakDInput
 
     InputProviders["oak1"] = Oak1Input
+    InputProviders["oakd"] = OakDInput
 except ImportError as ex:
     logging.info(f"DepthAI not installed: {ex}")
 
 try:
     from visiongraph.input.ZEDInput import ZEDInput
 
     InputProviders["zed"] = ZEDInput
```

## Comparing `visiongraph-0.1.56.dist-info/METADATA` & `visiongraph-0.1.57.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: visiongraph
-Version: 0.1.56
+Version: 0.1.57
 Summary: Visiongraph is a high level computer vision framework.
 Home-page: https://github.com/cansik/visiongraph
 Author: Florian Bruggisser
 Author-email: github@broox.ch
 License: MIT License
 Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
```

## Comparing `visiongraph-0.1.56.dist-info/RECORD` & `visiongraph-0.1.57.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 visiongraph/AsyncGraphNode.py,sha256=sGVtD16mFM0okNXW9wdEHVJ9zzd7tD4gLFIA2lP4Wpo,1758
 visiongraph/BaseGraph.py,sha256=q_UPq53arsmW2wnZMo-jD9bGby2WFNTWwSBMvSLsxfU,2806
 visiongraph/GraphNode.py,sha256=o2uAgN4WknZV1b3gMIKF3TUFRahRRu7AdXiuXMX4hYI,678
 visiongraph/Processable.py,sha256=bhe_zj7nT2xgmD0xYM_BEWMmDlAUtjfsi3jfdLhMb60,276
 visiongraph/VisionGraph.py,sha256=d5EkjL24hmRteBXfSqZD9xfglkM4bDUIZl4GFlhADcQ,2094
 visiongraph/VisionGraphBuilder.py,sha256=i0bn8HLb3yRACeblpk4yFtMJ6hY61R46mFL1Pdkw_Vg,1898
-visiongraph/__init__.py,sha256=XIxgOS9i3E21wxUi8Y88y8POgYirhPsuWbU_7h0lE5A,25160
+visiongraph/__init__.py,sha256=_htNZXu8qmEsDYRI63W3MqkE5ockP1RToWNRTjFUn-Y,25589
 visiongraph/cache/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/data/Asset.py,sha256=N43JxIuOjCmW9CropssmFaXvrtIFnQ382YvXT2nfdlM,368
 visiongraph/data/LocalAsset.py,sha256=7AIWlgdwaWHBiC0RbooqpWoRf68nQx3AT0wWdfJ4hk8,376
 visiongraph/data/RepositoryAsset.py,sha256=D9Vmjn4qKjtUdHHryRHVVKaZZg8jWExZX2frDNt2NwE,1099
 visiongraph/data/__init__.py,sha256=xhFDTiguCodQGCs0MvQvFZ6iN5MqFlyDqBA0ZWi69vw,386
 visiongraph/data/labels/COCO.py,sha256=4kP0wW4j16KIfdmCPhPNlQAnacZ3Etp2XC8LeECbe5A,2568
 visiongraph/data/labels/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -154,26 +154,28 @@
 visiongraph/external/motpy/tracker.py,sha256=S7Gtjkm1Nb9DEmqn9WyHK5hLQMgj20O6SrjD23TY-Gg,16512
 visiongraph/external/motpy/utils.py,sha256=Yed57TN_oc8BNXDhk1WMeJMJiltGJuIMrHnvPq_qBD4,652
 visiongraph/external/motrackers/Track.py,sha256=feiTQa_G6GH8fIc_3_rC7gEr2Jch9sn_QP6fXDNlhFQ,5312
 visiongraph/external/motrackers/Tracker.py,sha256=x9c5Yt8eSVmDdyZjh76bE1oWeTBRnP3yT3jyUDMOgMg,7486
 visiongraph/external/motrackers/__init__.py,sha256=oZ27LMiAAmB0_j5sMJ8wL-z66BGOtBzSn46bc2HV-0Q,184
 visiongraph/external/motrackers/utils/__init__.py,sha256=wfsRUA77UC208Y2lKT-usnynHzGVjH4_8BYqsY0UWh0,83
 visiongraph/external/motrackers/utils/misc.py,sha256=Q4QuYxw3nb6frDHRVREghVzRm9oWZFua3jDG_uBkTt8,8978
-visiongraph/input/AzureKinectInput.py,sha256=IMcavprAVyup27DbR8tGkaMgPVxx4kv1eS7uSE7rVeU,16500
+visiongraph/input/AzureKinectInput.py,sha256=Y1I67MWMy_V_CRQ3Z6LGz9qdC8r1AsE3r23xmTKN3os,17144
 visiongraph/input/BaseCamera.py,sha256=OwRLEdEsIoQ6P8xKp67MA8o_ikbdIHzrFE_EZIPQzfQ,3855
-visiongraph/input/BaseDepthCamera.py,sha256=sClNOdvZbw8UC9rUi4PYdHPOhWszBFsKQp0PdZIElRw,2352
+visiongraph/input/BaseDepthCamera.py,sha256=rfr5-DFOBL-80IjqhQmdpYr5xqKtN0nve02lZeOFnRg,3338
 visiongraph/input/BaseDepthInput.py,sha256=3nYxcLLLlPkT4oPewVQJMcLf8u3J5AABEW84qICY0mA,1309
 visiongraph/input/BaseInput.py,sha256=hgqyiHo5gz3yq91uzG-newww4DoNnbOnbnQK4LMpny0,4232
 visiongraph/input/CamGearInput.py,sha256=FVQROPlYVpVrk2CqVeg_bT83Z0UkN9GsCLj0lUg2SZs,1721
+visiongraph/input/DepthAIBaseInput.py,sha256=kZDL8GxvqRlWOqyhyTNagxNfGnsV2PWJrOl48OxSBuU,9093
 visiongraph/input/ImageInput.py,sha256=fX0sJcNcnd3vnu_2_23kpFPNr7goJNy7OIeERznTEjI,1493
-visiongraph/input/Oak1Input.py,sha256=ikTfWL0ihkgRVq2KFnyv2dcGtWwI34y-mtEAKmcc0Wo,8373
-visiongraph/input/RealSenseInput.py,sha256=7PTZl0Q69UQFntKC9jHQNZ40eMLZrSpXeIfnOQpz3Ss,19066
+visiongraph/input/Oak1Input.py,sha256=r-39MqsuO-Rz2uzVjMzrX4o5eQlQd8PZpy7ofzWx5IA,295
+visiongraph/input/OakDInput.py,sha256=bCXdKad-H0eBrc7n5xFBbI2NLnxJ0w-ZtjQAij1Gjh4,7425
+visiongraph/input/RealSenseInput.py,sha256=nTdtRhbPooVjUVuWJiKQJCcROjFIanAi13YVZKGJJvY,19468
 visiongraph/input/VideoCaptureInput.py,sha256=hupeQtiYPJyc610Hjpp8PleWQ1_Guvx0Xkvw0bOgmmE,5032
-visiongraph/input/ZEDInput.py,sha256=j0dxuMZLGsqhmyN96MNne7NndFpQhYm262zaUsRhUtA,4453
-visiongraph/input/__init__.py,sha256=RI0CA5wwSdVPJNEGd0lRGHIxcaIaY5vsF0gDAwbEX4o,1651
+visiongraph/input/ZEDInput.py,sha256=beNTRayN8OC3LtcrA4jyTBF3Fijax4eQsPuIKzdNJpI,4814
+visiongraph/input/__init__.py,sha256=SwVIv9t_VpDRy5NwzSH6stRdPsOhX1VCXXAscvg1Q-Y,1744
 visiongraph/model/CameraIntrinsics.py,sha256=8VwgAVjwMocZqoRsbpd2beYetI3_v1WpcM9jppirUJw,1887
 visiongraph/model/CameraStreamType.py,sha256=lYCzWM6blHEi_cwKo2U-xx8LqOIfbjvUJyAppRRTXMg,101
 visiongraph/model/DepthBuffer.py,sha256=YjvpgAIcdoXVB2QdVzZxplDlkW0lX1mxdgAPukc9E2Y,549
 visiongraph/model/VisionEngineModelLayer.py,sha256=rogzmsOxktiVXFskQEPzoSGHswVJ6LZtbstXN8SlOK4,266
 visiongraph/model/VisionEngineOutput.py,sha256=LdlYxbv8Rn5fnAnb7r8mDukBo2GWgYr3265lxOH-1A0,753
 visiongraph/model/_ImportStub.py,sha256=5kk04z_ji1TFi6IK0KlKditRTRvlIL7HCCF9Ztwplyw,135
 visiongraph/model/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -276,12 +278,12 @@
 visiongraph/util/OSUtils.py,sha256=B1wbK-iZMKCfVJf1j_0GGAror2VEDto1aikTBnhcEqw,195
 visiongraph/util/OpenVinoUtils.py,sha256=S5GPTf60VXCG9Sa5cnrvmxN1C6GDK9PqjjAqwqlLM5s,700
 visiongraph/util/PoseUtils.py,sha256=5G5YR5ywZ8k4mMGITQfXABCO3cA3O_gGUlDuIwXAsmI,5639
 visiongraph/util/ResultUtils.py,sha256=YZeBTTVI8qdT-cvQtdA31S5PTXXUTN5nZXkLs5ogUf0,1608
 visiongraph/util/TimeUtils.py,sha256=IVCLc5PtSYhmGB8hp6-oZMceva8Y6uU0K_1BDTZXGmE,2102
 visiongraph/util/VectorUtils.py,sha256=XA-DeZeruN7Sbzld7Knj-Ne4HOh2r7gK6k31nnm1EK8,3643
 visiongraph/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-visiongraph-0.1.56.dist-info/METADATA,sha256=WlnJZGU603gpw9mb0F9EfIKZvQ4NnI_lib1q1HjQu48,11923
-visiongraph-0.1.56.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-visiongraph-0.1.56.dist-info/entry_points.txt,sha256=eqGnTHtEVMYwfXVk1Z9MhC8O2N8wuqAbr0lisLmrkxs,20
-visiongraph-0.1.56.dist-info/top_level.txt,sha256=rMp8bfRr_CcL2T8juTpUUszIVf1_BFmagl0lhq3L16o,12
-visiongraph-0.1.56.dist-info/RECORD,,
+visiongraph-0.1.57.dist-info/METADATA,sha256=xTlbVA1s_7I_si0q6IWICDCPqiqal7FGDzx2vIrLucI,11923
+visiongraph-0.1.57.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+visiongraph-0.1.57.dist-info/entry_points.txt,sha256=eqGnTHtEVMYwfXVk1Z9MhC8O2N8wuqAbr0lisLmrkxs,20
+visiongraph-0.1.57.dist-info/top_level.txt,sha256=rMp8bfRr_CcL2T8juTpUUszIVf1_BFmagl0lhq3L16o,12
+visiongraph-0.1.57.dist-info/RECORD,,
```

